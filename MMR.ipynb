{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOD6Etaa9ST8gLb0z/xdmo0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lixKGSUtCkwm"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic datasets openai datamapplot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from huggingface\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"maartengr/arxiv_nlp\")[\"train\"]\n",
        "\n",
        "# Extract metadata\n",
        "abstracts = list(dataset[\"Abstracts\"])\n",
        "titles = list(dataset[\"Titles\"])"
      ],
      "metadata": {
        "id": "rB4zmo4OCqu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Create an embedding for each abstract\n",
        "embedding_model = SentenceTransformer('thenlper/gte-small')\n",
        "embeddings = embedding_model.encode(abstracts, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "HuiXy06TCtkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Create an embedding for each abstract\n",
        "embedding_model = SentenceTransformer('thenlper/gte-small')\n",
        "embeddings = embedding_model.encode(abstracts, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "4Fe7XahgCvx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from umap import UMAP\n",
        "\n",
        "# We reduce the input embeddings from 384 dimenions to 5 dimenions\n",
        "umap_model = UMAP(\n",
        "    n_components=5, min_dist=0.0, metric='cosine', random_state=42\n",
        ")\n",
        "reduced_embeddings = umap_model.fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "sWonYmNMC0LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hdbscan import HDBSCAN\n",
        "\n",
        "# We fit the model and extract the clusters\n",
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size=50, metric='euclidean', cluster_selection_method='eom'\n",
        ").fit(reduced_embeddings)\n",
        "clusters = hdbscan_model.labels_\n"
      ],
      "metadata": {
        "id": "QNJnsIr0C3gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "# Train our model with our previously defined models\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    verbose=True\n",
        ").fit(abstracts, embeddings)"
      ],
      "metadata": {
        "id": "rE0BtnSnC6AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic.representation import MaximalMarginalRelevance\n",
        "\n",
        "# Update our topic representations to MaximalMarginalRelevance\n",
        "representation_model = MaximalMarginalRelevance(diversity=0.5)\n",
        "topic_model.update_topics(abstracts, representation_model=representation_model)"
      ],
      "metadata": {
        "id": "BdTYYSX3C8no"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}